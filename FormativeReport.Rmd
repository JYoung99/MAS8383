---
title: 'Formative Practical Report: Summarising Multivariate Data and PCA'
author: "Jack Young"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(nclSLR)
library(ggplot2)
library(tidyr)
```

## 1 - Summarising the airpollution data

### (a) - Numerical and graphical summaries of the data

We start by selecting the `airpollution` dataset from our package. We can draw immediate insights using the summary function - as shown below, this shows us all of the variables in our data as well as some summary statistics for each variable:

```{r intro,message=FALSE}
summary(airpollution)
```

We can also visualize this information using histogram plots for each variable - 
these give us a little more insight as to how the values are distributed:
```{r boxplots, message=FALSE}
## Produce histograms of each variable in the airpollution data
ggplot(gather(airpollution), aes(value)) +
  geom_histogram(bins = 10) +
  facet_wrap(~key, scales = 'free_x')

```

As above, we see a variety of distributions present in our data. For example,
some seem to roughly follow a normal distribution, whereas others do not, such
as the `PM2` variable which appears to approximately follow an exponential
distribution.

We can check the dimensions of the data like so:
```{r dims}
dim(airpollution)
```

In the context of this dataset, this means that we have 80 observations, and 11
variables in total. We could produce a scatterplot matrix for our data, but this
may be impractical in this case given the large number of variables present. A
good alternative for this is to use a correlation heatmap instead, which
colour-codes variable pairs based on their correlation coefficient:

```{r pairs}
## Produce a correlation heatmap based on the data matrix
cor_matrix <- cor(airpollution)
corrplot::corrplot(cor_matrix)
```

For example, we can see here that there is a fairly strong correlation between
variables 'PERWH' and 'NONPOOR', however there are very few variables present
that show strong intercorrelation.

### (b) - Variation in the data

As stated in Section 1.4.4 in the course notes, we have the following two 
measures of multivariate scatter:

1. **Generalised variance:** $\text{det}(S) = \lvert S\rvert$, the determinant
of the sample covariance matrix,
2. **Total variation:** $\text{tr}(S)$, the trace of the sample covariance
matrix.

Using R, we can compute the sample covariance matrix $S$ by simply feeding
our data matrix to the `var` function like so:

```{r Variances}
## Compute the sample covariance matrix S
S <- var(airpollution)
S
```

Now we have computed $S$, we can find the generalized variance by computing the
determinant:

```{r genvar}
genvar <- det(S)
genvar
```
We can also compute the total variation by taking the trace of the matrix:
```{r totvar}
totvar <- sum(apply(airpollution, 2, var))
totvar
```

### (c) - Standardising the data matrix

An important practice in PCA is the standardizing of data - this means that
the features are scaled such that they are distributed around a mean of zero
with a standard deviation of one. We may then go ahead and compare covariances
for pairs of features in our data, but we shall first check that our assumptions
hold by performing standardization on the `airpollution` data.

```{r standard}
## Standardize the airpollution data
airpollution_standard <- scale(airpollution)
```

Now we have our standardized data, we can check that the sample mean vector is
composed of zeros:

```{r meanvec}
## Round values to 10 decimal places to account for rounding errors
standard_mean_vec <- round(colMeans(airpollution_standard),10)
standard_mean_vec
```

We must also check that the sample covariance matrix
is equal to the sample correlation matrix of the original `airpollution` data.
We can check these matrices are identical using the `all.equal` function:

```{r}
## Take the covariance matrix of the standardised data
standard_cov_matrix <- cov(airpollution_standard)
## Again accounting for rounding issues, check the new covariance matrix
## is equal to the original correlation matrix
all.equal(round(cor_matrix,10), round(standard_cov_matrix,10))
```

As we have now verified our assumptions made about the standardised data matrix,
we may proceed to perform PCA on the data.

## 2 - Principal Component Analysis

### (a) - Which matrix?

When examining the sample variances for the 11 variables in our data, we find
the following:

```{r variances}
## Take the individual variances for each variable in airpollution
apply(airpollution, 2, var)
```

Notice that, for example, the variance of the `PM2` variable is significantly
larger than that of the `LPOP` variable. As PCA is not
scale invariant, this could affect our analysis as if several components have
a larger mean/variance than others in the data, they will dominate our PCA if
based on our covariance matrix $S$. Therefore, we shall instead choose a PCA 
based on the spectral decomposition of the sample correlation matrix, which is
equivalent to performing the analysis on the standardised data.

### (b) - Performing PCA on the standardised data

Now we have decided to perform our analysis on the sample correlation matrix,
we can start our analysis.

```{r PCA}
## Perform PCA on the sample correlation matrix
pca_airpol <- prcomp(airpollution, scale = TRUE)
pca_airpol
```
To begin to draw some insights from our PCA, we can extract components
individually, like so:
```{r PCA components}
## Compute the variances of each principal component
pca_airpol$sdev^2

## Extract the loadings matrix
pca_airpol$rotation

```
By using the loadings matrix, we find that the first principal component is
given by:

$$
\begin{aligned}
  \texttt{PC1} = 0.261\texttt{SMIN} + 0.450\texttt{SMEAN} + 0.399\texttt{SMAX} + 0.313\texttt{PMIN}  + 0.387\texttt{PMEAN} + 0.252\texttt{PMAX} + \\0.240\texttt{PM2} + 0.207\texttt{PERWH} + 0.276\texttt{NONPOOR} + 0.106\texttt{GE65} + 0.265\texttt{LPOP}
\end{aligned}
$$

As we can see, the first principal component isn't particularly dominated by any
one of our variables here, as all of our coefficients fall between $+0.1$ and
$+0.5$. We find that the higher coefficients have been generally attributed to
sulphate and particulate readings however, so we may interpret our first
principal component as a weighted average of pollution rates. Cities with
higher readings of pollution will have larger scores for PC1, but more generally
cities with high values across the 11 variables will score highly here.

Moving on to the second principal component:

$$
\begin{aligned}
  \texttt{PC2} = 0.190\texttt{SMIN} - 0.013\texttt{SMEAN} - 0.134\texttt{SMAX} - 0.227\texttt{PMIN}  - 0.340\texttt{PMEAN} - 0.345\texttt{PMAX} + \\0.146\texttt{PM2} + 0.459\texttt{PERWH} + 0.365\texttt{NONPOOR} + 0.540\texttt{GE65} + 0.041\texttt{LPOP}
\end{aligned}
$$

The second principal component differs from the first in that it contains both
positive and negative coefficients for the variables. Generally speaking, the
demographic factors have positive coefficients, with the `GE65` variable the
largest in absolute value of these. On the other hand, the pollution-related
variables generally have been attributed with positive coefficients - especially
the particulate readings. Therefore, we could interpret that cities with lower
pollution rates and more white, less deprived and older populations will have
a high PC score for PC2, and vice versa. This principal component allows us to
contrast high pollution rates with our numerical demographic factors.

### (c) - How many Principal Components?

Now we have our principal components, we can decide how many to use. To do this,
we use Result 2.1 - that is, we can take the sum of the variances of the
principal components to be equal to the total variation in the original data.
Therefore, we may use the variance of one principal component divided by the
sum over all principal components to be the proportion of variation accounted
for by our one principal component. R, using the `summary` function calculates
the proportion of variance and cumulative proportion, as displayed below:

```{r variance proportion}
summary(pca_airpol)
```

We can also use a scree plot to help visualize this:

```{r scree}
plot(pca_airpol, type="lines", main="")
title(xlab="Component number")

```

Usually when carrying out an analysis such as this, we would look for a 'kink'
in our scree plot - i.e. where the gradient of our plot flattens out. Therefore,
with an appropriate threshold in mind, we notice that the first 4 PCs explain
around 74% of the total variation, and the remaining 6 components provide little
in comparison. Therefore with the goal of dimension reduction in mind, we could
probably disregard the final 6 for the purpose of our analysis.

### (d) - Plotting the first two components

We can plot the first principal component scores against each other, labelling
the points by the city they represent:

```{r 2compplot, fig.width=10, fig.height=8}
# Plot the first two principal components against each other
plot(pca_airpol$x[,1], pca_airpol$x[,2], xlab="First PC", ylab="Second PC")
# Add labels representing the cities
text(pca_airpol$x[,1], pca_airpol$x[,2], labels=rownames(airpollution_standard), cex=0.7, pos=3)
```

From this plot, we can begin to draw some insight from our data based on the
characteristics we managed to infer about each of the first two principal
components earlier.
For example, *CHARLESTON* stands out instantly, as we notice that it has the
highest score in the data for our first PC, yet the lowest score for PC 2.
Applying the interpretation we formulated in part (b), this would suggest that
this city has high rates of pollution present in the air, and that its
population is less white, more deprived and younger than most cities in our
data.
Now consider *JERSEYC* on the top right of the graph - this scores highly on
both the first and second PC axes. With a high PC 1 score, we infer that this
city generally had high scores across the 11 variables. However, with a high
score for PC 2, this interpretation may change. High PC 2 values indicate more
white, less deprived, and older populations, and as 5 out of 6 of our pollution
variables are negatively weighted in this PC, this would also suggest low
pollution levels in this city. Therefore combining these two interpretations,
it is suggested that this city in particular will have high values across the
board on our demographic factors outlined. However, as we have only taken the
first two principal components, which we found to represent just 52% of the
total variation in our data, we may have to take such interpretations with a
pinch of salt.
